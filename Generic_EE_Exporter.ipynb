{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Generic Earth Engine Exporter (GRID or POINTS from CSV)\n",
        "\n",
        "# Requirements: Google Colab + Earth Engine enabled\n",
        "# - Mounts Google Drive and writes CSVs there\n",
        "# - Datasets, dates, bands, mode\n",
        "# - MODE = \"GRID\"   => sample all pixels over regions (e.g., states) at each dataset's native resolution\n",
        "# - MODE = \"POINTS\" => sample at coordinates from a CSV (lon, lat[, id]); user SCALE_M is ignored\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import ee\n",
        "from google.colab import drive\n",
        "\n",
        "# 0) Google Drive + EE init\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_FOLDER = \"GEE_Exports_Generic\"  # change as needed\n",
        "drive_path = Path('/content/drive/My Drive/')\n",
        "\n",
        "# Authenticate & initialize EE\n",
        "ee.Authenticate()  # In Colab, follow the popup flow\n",
        "ee.Initialize(project='ee-your_project_id')  # <-- your EE project id\n",
        "\n",
        "# 1) Configuration\n",
        "\n",
        "# MODE: \"GRID\" or \"POINTS\"\n",
        "MODE = \"GRID\"\n",
        "\n",
        "# Dates - using inclusive start, exclusive end\n",
        "START_DATE = ''\n",
        "END_DATE   = ''\n",
        "\n",
        "# Sampling resolution (meters)\n",
        "# NOTE: Ignored in POINTS mode. In GRID mode we ALWAYS use each dataset's native/default scale.\n",
        "SCALE_M = 1000  # kept for backward compatibility; not used anymore\n",
        "\n",
        "# Dataset(s)\n",
        "DATASETS = {\n",
        "    # 1) DAYMET daily (~1 km)\n",
        "    \"DAYMET_V4\": {\n",
        "        \"collection_id\": \"NASA/ORNL/DAYMET_V4\",\n",
        "        \"default_bands\": ['prcp', 'tmin', 'tmax', 'vp', 'srad', 'dayl'],\n",
        "        \"note\": \"Daily Daymet; USA & parts of Canada/Mexico\",\n",
        "        \"default_scale\": 1000  # native ~1 km\n",
        "    },\n",
        "    # 2) ERA5-Land daily aggregated\n",
        "    \"ERA5_LAND_DAILY\": {\n",
        "        \"collection_id\": \"ECMWF/ERA5_LAND/DAILY_AGGR\",\n",
        "        \"default_bands\": [\n",
        "            'total_precipitation_sum',\n",
        "            'temperature_2m',\n",
        "        ],\n",
        "        \"note\": \"Daily aggregated ERA5-Land; global coverage\",\n",
        "        \"default_scale\": 11132  # ~0.1° ~= 11.1 km at equator, used consistently\n",
        "    }\n",
        "}\n",
        "\n",
        "# Choose which datasets and which bands from each to export.\n",
        "SELECTIONS = [\n",
        "    {\"name\": \"DAYMET_V4\", \"bands\": ['prcp', 'tmin', 'tmax']},\n",
        "    {\"name\": \"ERA5_LAND_DAILY\", \"bands\": ['total_precipitation_sum', 'temperature_2m']},\n",
        "]\n",
        "\n",
        "# GRID mode region config\n",
        "USE_CONUS_STATES = True\n",
        "STATE_EXCLUDE = ['']               # exclude list if needed\n",
        "ONLY_STATES = ['']        # subset if desired\n",
        "\n",
        "# POINTS mode CSV config\n",
        "POINTS_CSV = \"/content/drive/My Drive/SMAP_locations/coord_df.csv\"  # columns: lon, lat [, location_id]\n",
        "POINTS_LON_COL = \"lon\"\n",
        "POINTS_LAT_COL = \"lat\"\n",
        "POINTS_ID_COL  = \"location_id\"   # optional; auto-generated if missing\n",
        "\n",
        "# Export knobs\n",
        "EXPORT_FOLDER = DRIVE_FOLDER\n",
        "PER_STATE_FILE_IN_GRID_MODE = True  # one file per region (True) vs one global file (False)\n",
        "CLIENT_SIDE_CHUNK_SLEEP = 1         # seconds between starting export tasks\n",
        "\n",
        "# 2) Helpers\n",
        "\n",
        "def get_image_collection(ds_name, start_date, end_date, selected_bands=None):\n",
        "    \"\"\"Return ee.ImageCollection filtered by date and selected bands.\"\"\"\n",
        "    meta = DATASETS[ds_name]\n",
        "    col = ee.ImageCollection(meta[\"collection_id\"]).filterDate(start_date, end_date)\n",
        "    bands = selected_bands if selected_bands else meta[\"default_bands\"]\n",
        "    col = col.select(bands)\n",
        "    return col, bands, meta\n",
        "\n",
        "def add_lonlat(feature):\n",
        "    \"\"\"Add lon/lat properties from geometry.\"\"\"\n",
        "    coords = ee.Feature(feature).geometry().coordinates()\n",
        "    return feature.set({'lon': coords.get(0), 'lat': coords.get(1)})\n",
        "\n",
        "def sample_ic_over_region_grid(ic, region_geom, scale_m):\n",
        "    \"\"\"Sample every pixel center at given scale over region for every image in collection.\"\"\"\n",
        "    def sample_one_image(img):\n",
        "        date_str = img.date().format('YYYY-MM-dd')\n",
        "        fc = (\n",
        "            img.sample(region=region_geom,\n",
        "                       scale=scale_m,\n",
        "                       geometries=True)\n",
        "              .map(lambda f: f.set(\"date\", date_str))\n",
        "              .map(add_lonlat)\n",
        "        )\n",
        "        return fc\n",
        "    return ic.map(sample_one_image).flatten()\n",
        "\n",
        "def sample_ic_at_points(ic, points_fc, scale_m):\n",
        "    \"\"\"Sample an image collection at given point features (lon/lat).\"\"\"\n",
        "    def sample_one_image(img):\n",
        "        date_str = img.date().format('YYYY-MM-dd')\n",
        "        fc = (\n",
        "            img.sampleRegions(collection=points_fc,\n",
        "                              scale=scale_m,\n",
        "                              geometries=True)\n",
        "              .map(lambda f: f.set(\"date\", date_str))\n",
        "              .map(add_lonlat)\n",
        "        )\n",
        "        return fc\n",
        "    return ic.map(sample_one_image).flatten()\n",
        "\n",
        "def export_fc_to_drive(fc, description, file_prefix, folder):\n",
        "    \"\"\"Start a Drive export task for the given FeatureCollection.\"\"\"\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=fc,\n",
        "        folder=folder,\n",
        "        description=description,\n",
        "        fileNamePrefix=file_prefix,\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"  ✅ Export started: {description}  ->  Drive/{folder}/{file_prefix}*.csv\")\n",
        "\n",
        "def build_points_fc_from_csv(path, lon_col=\"lon\", lat_col=\"lat\", id_col=None):\n",
        "    \"\"\"Read CSV and build ee.FeatureCollection of point features.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    if lon_col not in df.columns or lat_col not in df.columns:\n",
        "        raise ValueError(f\"CSV must contain '{lon_col}' and '{lat_col}' columns.\")\n",
        "    if id_col and id_col not in df.columns:\n",
        "        print(f\"  (Info) '{id_col}' not found, will auto-generate IDs.\")\n",
        "\n",
        "    feats = []\n",
        "    for i, row in df.iterrows():\n",
        "        lon = float(row[lon_col])\n",
        "        lat = float(row[lat_col])\n",
        "        props = {}\n",
        "        if id_col and id_col in df.columns:\n",
        "            # keep as string but safe\n",
        "            props[\"site_id\"] = str(row[id_col]).strip()\n",
        "        else:\n",
        "            props[\"site_id\"] = f\"pt_{i}\"\n",
        "        feat = ee.Feature(ee.Geometry.Point([lon, lat]), props)\n",
        "        feats.append(feat)\n",
        "    return ee.FeatureCollection(feats)\n",
        "\n",
        "def get_conus_states_fc(only_states=None, exclude_states=None):\n",
        "    \"\"\"Return FeatureCollection of CONUS states (optionally filtered).\"\"\"\n",
        "    states = ee.FeatureCollection(\"TIGER/2018/States\")\n",
        "    if exclude_states:\n",
        "        states = states.filter(ee.Filter.inList(\"NAME\", exclude_states).Not())\n",
        "    if only_states:\n",
        "        states = states.filter(ee.Filter.inList(\"NAME\", only_states))\n",
        "    return states\n",
        "\n",
        "# 3) Main logic per MODE\n",
        "\n",
        "print(f\"\\nMode: {MODE}\")\n",
        "print(f\"Date range: [{START_DATE}, {END_DATE})\")\n",
        "print(f\"Export folder: {EXPORT_FOLDER}\")\n",
        "if MODE.upper() == \"POINTS\":\n",
        "    print(\"Scale: Ignored in POINTS mode (using dataset-native scale internally).\")\n",
        "else:\n",
        "    print(\"Scale: Using dataset-native/default scale (original resolution).\")\n",
        "\n",
        "if MODE.upper() == \"GRID\":\n",
        "    # Build region FeatureCollection\n",
        "    if USE_CONUS_STATES:\n",
        "        regions_fc = get_conus_states_fc(\n",
        "            only_states=ONLY_STATES if ONLY_STATES else None,\n",
        "            exclude_states=STATE_EXCLUDE\n",
        "        )\n",
        "        region_list = regions_fc.aggregate_array(\"NAME\").getInfo()\n",
        "        print(f\"Regions (states) to process: {region_list}\")\n",
        "    else:\n",
        "        raise ValueError(\"Set USE_CONUS_STATES=False only if you provide your own regions FC.\")\n",
        "\n",
        "    # Loop datasets (always at native/default resolution)\n",
        "    for sel in SELECTIONS:\n",
        "        ds_name = sel[\"name\"]\n",
        "        bands = sel.get(\"bands\")\n",
        "        ic, used_bands, meta = get_image_collection(ds_name, START_DATE, END_DATE, bands)\n",
        "        scale_for_ds = meta.get(\"default_scale\", 1000)  # ORIGINAL RESOLUTION\n",
        "\n",
        "        print(f\"\\n=== DATASET: {ds_name} ({meta['collection_id']}) ===\")\n",
        "        print(f\"Bands: {used_bands}\")\n",
        "        print(f\"Sampling scale (native/default, m): {scale_for_ds}\")\n",
        "\n",
        "        if PER_STATE_FILE_IN_GRID_MODE:\n",
        "            for st_name in region_list:\n",
        "                print(f\"\\n-- Region: {st_name} --\")\n",
        "                geom = regions_fc.filter(ee.Filter.eq(\"NAME\", st_name)).geometry()\n",
        "                samples = sample_ic_over_region_grid(ic, geom, scale_for_ds)\n",
        "\n",
        "                safe_st = st_name.replace(' ', '_').lower()\n",
        "                file_prefix = f\"{ds_name.lower()}_{safe_st}_{START_DATE.replace('-','')}_{END_DATE.replace('-','')}\"\n",
        "                desc = f\"{ds_name}_{safe_st}_{START_DATE}_to_{END_DATE}_grid\"\n",
        "                export_fc_to_drive(samples, desc, file_prefix, EXPORT_FOLDER)\n",
        "                time.sleep(CLIENT_SIDE_CHUNK_SLEEP)\n",
        "        else:\n",
        "            print(\"\\n-- Merging all regions into a single export --\")\n",
        "            merged = ee.FeatureCollection([])\n",
        "            def per_region(st_name):\n",
        "                geom = regions_fc.filter(ee.Filter.eq(\"NAME\", st_name)).geometry()\n",
        "                fc = sample_ic_over_region_grid(ic, geom, scale_for_ds)\n",
        "                return fc.map(lambda f: f.set(\"region_name\", st_name))\n",
        "\n",
        "            for st_name in region_list:\n",
        "                merged = merged.merge(per_region(st_name))\n",
        "\n",
        "            file_prefix = f\"{ds_name.lower()}_regions_{START_DATE.replace('-','')}_{END_DATE.replace('-','')}\"\n",
        "            desc = f\"{ds_name}_regions_{START_DATE}_to_{END_DATE}_grid\"\n",
        "            export_fc_to_drive(merged, desc, file_prefix, EXPORT_FOLDER)\n",
        "            time.sleep(CLIENT_SIDE_CHUNK_SLEEP)\n",
        "\n",
        "elif MODE.upper() == \"POINTS\":\n",
        "    # Build points FC from CSV\n",
        "    print(f\"Reading points CSV: {POINTS_CSV}\")\n",
        "    points_fc = build_points_fc_from_csv(\n",
        "        POINTS_CSV, lon_col=POINTS_LON_COL, lat_col=POINTS_LAT_COL, id_col=POINTS_ID_COL\n",
        "    )\n",
        "    npts = points_fc.size().getInfo()\n",
        "    print(f\"Points loaded: {npts}\")\n",
        "\n",
        "    # Loop datasets (SCALE_M IGNORED; use native/default)\n",
        "    for sel in SELECTIONS:\n",
        "        ds_name = sel[\"name\"]\n",
        "        bands = sel.get(\"bands\")\n",
        "        ic, used_bands, meta = get_image_collection(ds_name, START_DATE, END_DATE, bands)\n",
        "        scale_for_ds = meta.get(\"default_scale\", 1000)  # use dataset-native/default\n",
        "\n",
        "        print(f\"\\n=== DATASET: {ds_name} ({meta['collection_id']}) ===\")\n",
        "        print(f\"Bands: {used_bands}\")\n",
        "        print(f\"Sampling scale in POINTS mode (native/default, m): {scale_for_ds} (user SCALE_M ignored)\")\n",
        "\n",
        "        samples = sample_ic_at_points(ic, points_fc, scale_for_ds)\n",
        "\n",
        "        file_prefix = f\"{ds_name.lower()}_points_{START_DATE.replace('-','')}_{END_DATE.replace('-','')}\"\n",
        "        desc = f\"{ds_name}_points_{START_DATE}_to_{END_DATE}\"\n",
        "        export_fc_to_drive(samples, desc, file_prefix, EXPORT_FOLDER)\n",
        "        time.sleep(CLIENT_SIDE_CHUNK_SLEEP)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"MODE must be 'GRID' or 'POINTS'.\")\n",
        "\n",
        "print(\"\\nAll export tasks have been submitted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPAxer0a7xNg",
        "outputId": "caca0708-5ba5-4f2c-988a-fbe555d2a2b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Mode: GRID\n",
            "Date range: [2018-01-01, 2018-07-16)\n",
            "Export folder: GEE_Exports_Generic\n",
            "Scale: Using dataset-native/default scale (original resolution).\n",
            "Regions (states) to process: ['Arizona']\n",
            "\n",
            "=== DATASET: DAYMET_V4 (NASA/ORNL/DAYMET_V4) ===\n",
            "Bands: ['prcp', 'tmin', 'tmax']\n",
            "Sampling scale (native/default, m): 1000\n",
            "\n",
            "-- Region: Arizona --\n",
            "  ✅ Export started: DAYMET_V4_arizona_2018-01-01_to_2018-07-16_grid  ->  Drive/GEE_Exports_Generic/daymet_v4_arizona_20180101_20180716*.csv\n",
            "\n",
            "=== DATASET: ERA5_LAND_DAILY (ECMWF/ERA5_LAND/DAILY_AGGR) ===\n",
            "Bands: ['total_precipitation_sum', 'temperature_2m']\n",
            "Sampling scale (native/default, m): 11132\n",
            "\n",
            "-- Region: Arizona --\n",
            "  ✅ Export started: ERA5_LAND_DAILY_arizona_2018-01-01_to_2018-07-16_grid  ->  Drive/GEE_Exports_Generic/era5_land_daily_arizona_20180101_20180716*.csv\n",
            "\n",
            "All export tasks have been submitted.\n"
          ]
        }
      ]
    }
  ]
}